---

layout: page

title: matrix methods

description: notes from linear algebra

img:

importance: 4

category: work

---

<style>
    body {
      font-size: 16px; /* Adjust the font size as needed */
      line-height: 1; /* Adjust the line height as needed */
    }
    p {
      margin-bottom: 8px; /* Adjust the margin bottom for paragraphs as needed */
    }
    .math {
      font-size: 15px; /* Adjust the font size for math expressions as needed */
    }
</style>


<br><br>

<h2> Vectors </h2>
<br><br>
<h4>Canonical Unit Vectors (standard basis vectors)</h4>

\( \mathbb{e}_i\) = where the i-th position is 1 and all other positions are 0. For example, in \( \mathbb{R}^3 \):
\[
\mathbf{e}_1 = (1, 0, 0), \quad \mathbf{e}_2 = (0, 1, 0), \quad \mathbf{e}_3 = (0, 0, 1)
\]

Any vector \( \mathbf{v} \in \mathbb{R}^n \) can be expressed as:
\[
\mathbf{v} = v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + \cdots + v_n \mathbf{e}_n
\]

<br><br><br>
<h4>Inner Products (dot products)</h4>
\[ a \cdot b = a^T b \]

 
<b>Properties: </b><br>
1. \( b^T a = a^T b \) <br><br>
2. \( (\gamma a)^T b = \gamma (a^T b)\)
<br> scaling <b>a</b> then taking dot product with <b>b</b> is same as taking the dot product of <b>a</b> and <b>b</b> then scaling it <br><br>
3. \( (a + b)^T c = a^T c + b^T c \)
<br> the dot product of <b>c</b> with the sum of <b>a</b> and <b>b</b> is the same as the dot product of (a and c) plus (b and c) <br><br>

<br>
<b>Operations: </b><br>
1. Picking out the i'th element: \( \quad e_{i}^T a = a_{i}   \) <br><br>
2. Sum of elements: \( \quad 1_n a = a_1 + a_2 + a_3 ... a_n    \)  <br><br>
3. Sum of squares: \( \quad  a^T a = a_{1}^2 + a_{2}^2 + a_{3}^2 ... a_{n}^2  \) \( \quad \quad a^T a = 0 \quad \text{iff} \quad a = 0 \) <br><br>

<br><br><br>
<h2> Linear Functions </h2>
Satisfies superposition property (both homogeneity + additivity): <br><br>
<b>
1. Homogeneity: \( f(ax) = a f(x) \) <br>
2. Additivity: \( f(x + y) = f(x) + f(y) \) </b><br><br>

<b> Any linear function can be represented as a dot product \(f(x) = a^T x \) and any dot product function is linear (for some fixed vector a)</b> <br><br>
\[ f(c_1 x + c_2 y) = \]
\[ a^T (c_1 x + c_2 y) = a^T (c_1 x) + a^T (c_2 y) = c_1(a^T x) + c_2(a^T y) \]
\[ = c_1 f(x) + c_2 f(y) \]



<h4>Affine Functions</h4>
Affine functions are linear functions \( f(x) = a^T x \) but offset by some b (linear always go through (0,0), affine functions need not):
\[ f(x) = a^T x + b \]

<h4>Taylor Approximation</h4>
A first order taylor series is a linear approximation of the function f. <br>
Using a nearby point z, we can approximate f(x) using:
\[ \hat{f}(x) = f(z) + \Delta f(z)(x - z) \]


<br><br><br>
<h2> Norm and Distance </h2>
\[ ||x|| = \sqrt{x_{1}^2 + x_{2}^2 ...} = \sqrt{x^T x} \]
\[ ||x||^2 = \textbf{sum of squares} = x_{1}^2 + x_{2}^2 ... = x^T x \]
Properties: <br><br>
1. \( ||cx|| = |c|||x|| \) <br>
2. \( ||x + y|| \leq ||x|| + ||y|| \)<br>
3. \( ||x|| \geq 0 \)<br>
4. \( ||x|| = 0 \quad \) iff \( \quad x = 0 \)<br><br><br>


Mean square value: \( \frac{||x||^2}{n} \) <br>
Root Mean square value (RMS): \( \frac{||x||}{\sqrt{n}} \quad \) (i.e., typical value of \( |x_i| \))<br><br><br>

Block (stacked) Vectors: <br>
\[ ||(a,b,c)||^2 = a^T a + b^T b + c^T c = ||a||^2 + ||b||^2 + ||c||^2 \]

<br><br>
<b>Chebyshev's Inequality: </b><br><br>
- Judges the size of a vector <br>
- Most numbers in a vector can't be much bigger than its RMS <br><br>

Suppose k elements in vector x is larger than some a. Then: <br>
- Number of elements \( |x_i| > a \) is no more than \( ||x||^2 / a^2 \)<br>
- The percentage of elements \( |x_i| \geq a RMS(x) \)  is  \( \leq \frac{1}{a^2} \) <br>

\[ \frac{||x||^2}{a^2} \geq k \quad \text{or} \quad \frac{k}{n} \leq (\frac{RMS(x)}{a})^2 \]<br>

<b>Distance: </b><br><br>
Distance between a and b is: \( || a - b || \) <br><br>

<b>Triangle Inequality</b>: Sum of twox sides must be greater than or equal to third side<br>
<img src = "/assets/tri.png" width = 200px>
<br><br><br>

<b>Cauchy Schwartz Inequality: \( |a \cdot b| \leq ||a||||b|| \) </b><br><br>
<img src = "/assets/cauchy.png" width = 200px>
<br><br><br><br>

Average of \( x = \frac{ x_1 + x_2 + ...}{n} = \frac{1^T x }{n} \) <br><br>
Demeaned vector \( \tilde{x} = x - avg(x)1 \) <br><br>
Standard deviation of \( x = rms(\tilde{x}) = \frac{||x - avg(x)1|| }{\sqrt{n}} \) <br><br>
\( rms(x)^2 = avg(x)^2 + std(x)^2 \) <br><br>
Standardized vector \( z = \frac{\tilde{x}}{std(x)}\) <br><br><br>

<b>Example in Finance:</b> <br>
x = time series of returns <br>
avd(x) = average return <br>
std(x) = volatility <br><br>

Risk-Reward Plot: <br>
<img src = "/assets/finance.png" width = 200px>


<br><br><br>
<h2> Angles </h2>
\[\theta = \cos^{-1}\left(\frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}\right)\]
<img src = "/assets/angle.png" width = 700px>



<br><br><br><br><br>
<h2>Linear Independence</h2>
<br><br>
Vectors \( \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\} \) are linearly independent if \( c_1, c_2, \dots, c_k \) <b>= 0</b> is the <b>ONLY</b> solution to:
<br><br>
\[
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \dots + c_k \mathbf{v}_k = \mathbf{0}.
\]


<br><br><br>
<h4>Basis</h4>
<br>
A set of vectors \( \{\mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n\} \) forms a basis for a vector space \( V \) if:
<br><br>
<ul>
    <li>The vectors are linearly independent.</li>
    <li>The vectors span the whole space \( V \) (i.e., any vector in \( V \) can be expressed as a linear combination of the basis vectors)</li>
</ul>

Ex: The vectors \((0,1)\) and \((1,0)\) span \(\mathbb{R}^2 \)


<br><br><br><br><br>
<h4>Orthonormal Expansion</h4><br>
- Orthonormal vectors are orthogonal and its norm is 1 <br><br>
If \( \{\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_n\} \) is an orthonormal basis for a vector space \( V \), any vector \( \mathbf{v} \in V \) can be expressed as:
<br><br>
\[
\mathbf{v} = \sum_{i=1}^n (\mathbf{v} \cdot \mathbf{a}_i) \mathbf{a}_i,
\]

<br><br><br>

<h4>Gram-Schmidt Process</h4>
<br>
Orthogonal vectors means they are linearly independent but <b>linearly independent vectors DOES NOT indicate they are orthogonal</b> <br>

<br> Gram-Schmidt allows us to construct an orthonormal basis from a set of linearly independent vectors <br><br>

Given a set of linearly independent vectors \( \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\} \), the Gram-Schmidt process generates an orthonormal set \( \{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\} \) as follows:
<br><br>
\[ \mathbf{u}_1 = \frac{\mathbf{v}_1}{\|\mathbf{v}_1\|} \]

\[ \mathbf{u}_2 = \frac{\mathbf{v}_2 - (\mathbf{v}_2 \cdot \mathbf{u}_1) \mathbf{u}_1}{\| \mathbf{v}_2 - (\mathbf{v}_2 \cdot \mathbf{u}_1) \mathbf{u}_1 \|} \]
\[
\mathbf{u}_3 = \frac{\mathbf{v}_3 - (\mathbf{v}_3 \cdot \mathbf{u}_1) \mathbf{u}_1 - (\mathbf{v}_3 \cdot \mathbf{u}_2) \mathbf{u}_2}{\| \mathbf{v}_3 - (\mathbf{v}_3 \cdot \mathbf{u}_1) \mathbf{u}_1 - (\mathbf{v}_3 \cdot \mathbf{u}_2) \mathbf{u}_2 \|}
\]
<br><br>
This process continues for all \( \mathbf{v}_i \) to produce orthonormal vectors.
<br><br><br><br>


<h2>Matrices</h2>
<br>
<h5>Lower Triangular Matrices</h5>
\[
\begin{bmatrix}
a_{11} & 0      & 0      & 0      \\
a_{21} & a_{22} & 0      & 0      \\
a_{31} & a_{32} & a_{33} & 0      \\
a_{41} & a_{42} & a_{43} & a_{44}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
=
\begin{bmatrix}
a_{11}x_1 \\
a_{21}x_1 + a_{22}x_2 \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4
\end{bmatrix}
\]

<br><br>
<h5>Difference Matrices</h5>
\[
\begin{bmatrix}
-1 & 1 &  0  &  0  \\
0 &  -1 & 1  &  0  \\
0 &  0 &  -1  & 1
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}
=
\begin{bmatrix}
x_2 - x_1 \\
x_3 - x_2 \\
x_4 - x_3
\end{bmatrix}
\]



<br><br>
<h5>Rotation Matrices</h5>
A 2D rotation matrix for an angle $\theta$ is given by:
\[
    R(\theta) = \begin{bmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
    \end{bmatrix}
\]

<br><br>
<h5>Incidence Matrix</h5>
<img src = "/assets/incidence.png" width = 300px><br>
For a simple undirected graph with m edges and n nodes, the incidence matrix \( A \in \mathbb{R}^{nodes \times edges} \) is defined as:
\[
    A_{i,j} = \begin{cases}
        1, & \text{if edge } j \text{ incoming to } i \\
        -1, & \text{if edge } j \text{ leaving } i \\
        0, & \text{no connection}
    \end{cases}
\]

<br><br>
<h5>Incidence Matrix</h5>
A simple example of a 1D convolution smoothing matrix with a 3-point moving average kernel is:
\[
    C = \begin{bmatrix}
        \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & \dots & 0 \\
        0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & \dots & 0 \\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \dots & \frac{1}{3} & \frac{1}{3} & \frac{1}{3}
    \end{bmatrix}
\]


<br><br><br><br><br>
<h2>Linear Mapping</h2>
A <b>linear</b> mapping \( f: \mathbb{R}^n \to \mathbb{R}^m \) can be represented by a matrix \( A_{m \times n} \) such that for any vector \( x \in \mathbb{R}^n \):
\[
    f(x) = Ax
\]
<b> 1. f will always send 0 to 0 </b><br>
If f(0) \( \neq \) 0, f is NOT linear <br>
If f(0) \(=\) 0, f may or may not be linear
<br><br>
2. \( f(\alpha x + \beta y) = A(\alpha x + \beta y) = \alpha Ax + \beta Ay \)<br><br>
<b> 3. If f does not have a matrix representation, it is NONLINEAR</b><br><br><br><br>


<h4>Finding A Given x</h4>
For an \(n\)-vector \(\mathbf{x} = (x_1, \dots, x_n)\), we use the standard unit vectors to write:
\[
\mathbf{x} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \cdots + x_n \mathbf{e}_n
\]
Since \(f\) is linear, we have:
\[
f(\mathbf{x}) = x_1 f(\mathbf{e}_1) + x_2 f(\mathbf{e}_2) + \cdots + x_n f(\mathbf{e}_n)
\]

Now, we put together the \(m \times n\) matrix \(A\) column by column:
\[
A = \left[ f(\mathbf{e}_1) \,\, f(\mathbf{e}_2) \,\, \cdots \,\, f(\mathbf{e}_n) \right]
\]
Thus, we can write:
\[
f(\mathbf{x}) = x_1 f(\mathbf{e}_1) + x_2 f(\mathbf{e}_2) + \cdots + x_n f(\mathbf{e}_n) = A
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
\]



<h4>Price-Demand Elasticity Model</h4>

- Prices are given by an \( n \)-vector \( p \).<br>
- Demand (e.g., units sold) is given as an \( n \)-vector \( d \).<br>
- The fractional (percent) change in price of good \( i \) is:
    \[
    \delta_{price_i} = \frac{p_i^{\text{new}} - p_i}{p_i}
    \]
    The fractional (percent) change in demand of good \( i \) is:
    \[
    \delta_{dem_i} = \frac{d_i^{\text{new}} - d_i}{d_i}
    \]


The price-demand elasticity model assumes:
\[
\delta_{dem} = E \delta_{price}
\]

\[
E =
\begin{bmatrix}
-0.3 & 0.1 & \cdots \\
\vdots & \vdots & \vdots \\
\vdots & \vdots & \vdots
\end{bmatrix}
\]

\[
\delta_{dem_1} = -0.3 \delta_{price_1} + 0.1 \delta_{price_2} + \cdots
\]

Interpretation:
If price of item 1 decreases by 30%, price of item 2 increases by 10%

<br><br><br><br><br>

<h4>Balancing Chemical Equations</h4>
The given redox reaction \(Ra = Pb\), solve for a and b:<br><br>
\[
a_1 \text{Cr}_2\text{O}_7^{2-} + a_2 \text{Fe}^{2+} + a_3 \text{H}^+
\rightarrow b_1 \text{Cr}^{3+} + b_2 \text{Fe}^{3+} + b_3 \text{H}_2\text{O}
\]
<br><br>
<center>
\(
R =
\begin{bmatrix}
2 & 0 & 0 \\
7 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
-2 & 2 & 1
\end{bmatrix}
\quad \quad \)


\(
P =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 \\
0 & 0 & 2 \\
3 & 3 & 0
\end{bmatrix}
\)
<center>
