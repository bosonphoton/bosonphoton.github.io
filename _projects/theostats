---
layout: page

title: theoretical statistics

description: course notes from STATS200

img:

importance: 3

category: work
---
<br><br>

<h2>I. Probability Review</h2><br>
<h4> Laws of Probability Theory </h4>

(1) De Morgans Laws: <br>
$$(A \cup B \cup C)^c = A^c \cap B^c \cap C^c$$

(2) Law of Total Probability: <br>
- prob of event A occuring is equal to sum of event A occurring in all possible scenarios
$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$$

(3) Inclusion-Exclusion Principle: <br>
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

<b>Conditional Probability:</b> <br>
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
$$P(A \cap B) = P(A|B)P(B) = {}^* P(B|A)P(A) $$
<center> * Bayes Theorem! </center><br>

(1) Marginal Independence: <br>
- A and B are marginally independent if: <br>
$$P(A|B) = P(A)$$
$$P(A \cap B) = P(A)P(B)$$

(2) Conditional Independence: <br>
- A1 and A2 are conditionally independent given B if:
$$P(A_1 \cap A_2 | B) = P(A_1|B)P(A_2|B)$$



<br><br>
<h4>Random Variables & Distributions</h4>

<b>CDFs and PDFs: </b> <br>
- By FTC, the PDF is equal to the derivative of the CDF <br>
- Integrating the PDF from -âˆž to x gives us the CDF
$$F_X(x) = P(X \leq x)  = \int_{-\infty}^{x} f(t) \, dt$$
$$f_X(x) = \frac{d}{dx} F(x)$$


<b>Expectation: </b> <br>
$$\mathbb{E}[X] = \sum_{i} x_i P(X = x_i) \quad \text{(for discrete variables)}$$
$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx \quad \text{(for continuous variables)}$$
(1) Linearity Rule:
$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$
(2) Sum Rule:
$$\mathbb{E}\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} \mathbb{E}[X_i]$$

(3) Expectation of a Function: <br>
\[ E[g(X)] = \sum_{x} g(x) P(X = x) \]
\[ E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) \,dx \]


(4). Independence:
\[ \text{If X and Y are independent} \rightarrow \quad E[XY] = E[X] \cdot E[Y] \]




<b>Variance: </b> <br>

$$\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

$$\text{Var}(cX) = c^2 \text{Var}(X)$$

$$\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab\text{Cov}(X,Y)$$

<br><br>
<h4>Joint Variables</h4>
- <b>Marginal Densities</b> tell us the probability of one variable by ignore the other (if given a joint dist. of weight and height, the marginal of weight tells us info on weight regardless of height)
<br>
- <b>Conditional Densities</b> tell us the probability of one variable given that you know the value of the other (if height is 5'6, whats prob that weight is 110?)

<br><br>
<b>Discrete</b>: <br>

(1) Joint PMF:
\[ P(X = x, Y = y) = p_{X,Y}(x,y) \]

(2) Joint CDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \sum_{x' \leq x} \sum_{y' \leq y} p_{X,Y}(x', y') \]

(3) Marginal PMFs can be obtained from the joint PMF:
\[ p_X(x) = \sum_{y} p_{X,Y}(x,y) \]
\[ p_Y(y) = \sum_{x} p_{X,Y}(x,y) \]

<br>
<b>Continuous</b>: <br>

(1) Joint CDF is obtained by integrating the PDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \iint_{-\infty}^{(x,y)} f_{X,Y}(u,v) \,du\,dv \]

(2) Marginals obtained by integrating over opposite variable:
\[ f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dy \]
\[ f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dx \]


<br>
<b>Covariance: </b> <br>
\[ \text{Cov}(X, Y) = E\left[(X - E[X])(Y - E[Y])\right] \]

\[ \text{Cov}(X, Y) = E[XY] - E[X]E[Y] \]


(1) Covariance of a Constant:
\[ \text{Cov}(X, c) = 0 \]

(2) Covariance of a Random Variable with Itself:
\[ \text{Cov}(X, X) = \text{Var}(X) \]

(3) Covariance of a Linear Combination:
\[ \text{Cov}(aX + bY, Z) = a \, \text{Cov}(X, Z) + b \, \text{Cov}(Y, Z) \]

(4) Correlation:
\[ \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \quad \sigma = \text{sd} \quad \sigma^2 = \text{var} \]



<br><br>
<h4>Moment Generating Functions (MGFs)</h4>
MGFs can help us find the distributions of SUMS of independent RVs. <br><br>
(1) Zero'th moment is always = 1 <br>
(2) First moment is always = \( E(X) \) <br>
(3) Second moment is \( E(X^2) \) and can tell us the variance \( \sigma^2 = E(X^2) - [E(X)]^2 \) <br>


\[ M_X(t) = \mathbb{E}[e^{tX}] = \begin{cases}
\sum e^{t x} p(x)  \quad \text{(discrete)} \\
\\
\int_{}^{} e^{t x} f(x) \, dx \quad \text{(continuous)}
\end{cases}
\]

(1) MGF of the sum of independent RVs is the product
\[ M_{X_1 + X_2 + \cdots + X_n}(t) = M_{X_1}(t) M_{X_2}(t) \cdots M_{X_n}(t) \]
<center>  because </center>
\[ \mathbb{E}[e^{t(X_1 + X_2)}] = \mathbb{E}[e^{tX_1}] \cdot \mathbb{E}[e^{tX_2}] \]


(2) The k'th Derivative is the k'th Moment:
\[\mathbb{E}[X^k] = M_X^{(k)}(0) \]


(3) Continuity Theorem: <br>
<center>X_n and X are real-valued random variables:</center>

\[ X_n \xrightarrow{d} X \quad \text{as} \quad n \to \infty \quad (\text{convergence in distribution}) \]

<center>if and only if </center>

\[ M_{X_n}(t) \to M_X(t) \quad \text{as} \quad n \to \infty \quad \text{for all} \quad t. \]


<br><br>
<h4>Transformations of Random Variables</h4>
(1) We know some pdf \( f_X (x) \) <br>
(2) We know some function \( Y = g(X) \) (where g is continuous and strictly monotone) <br>
(3) We want to find the pdf \( f_Y (y) \) <br>
(4) Find the inverse of \( Y = g(X) \), so solve for X <br>
(5) Find the derivative of (4) <br>
(6) Plug in \( f_Y (y) = f_X (4) * |5| \)
<br><br>






<b> Application: Generating iid RVs ~F from Uniform RVs! </b> <br>
(1) Lets say I want to generate X1, X2,... Xn iid from some general distribution function F (given F is continuous and strictly increasing) <br>
(2) We can sample U1, U2... Un iid from Unif(0,1) <br>
(3) Then generate Xi = F^-1(Ui) <br><br>


Choose the distribution \( X \) you want to sample from (e.g., exponential, uniform, normal, etc.).

Determine the CDF \( F_X(x) \) of the desired distribution.

Set the uniform random variable \( U \) equal to the CDF:
\[U = F_X(x)\]

Rearrange the equation to solve for \( x \) in terms of \( U \):
\[x = F_X^{-1}(U)\]

Use a random number generator to obtain values of \( U \) and apply the above equation to generate samples from the desired distribution. Pretty damn cool if you ask me.

<br><br><br><br>
<h4>Central Limit Theorem (CLT) & Law of Large Numbers (LLN)</h4>

(1) <b>CLT</b>: The sum of a large number of i.i.d RV's tend to follow a normal distribution. AKA "convergence in distribution" to a normal. <br>
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \quad Var(\bar{X}_n) = \frac{\sigma^2}{n} \]
\[ \sqrt{n} ( \bar{X}_n - \mu) \quad \xrightarrow{d} \quad \mathcal{N}(0, \sigma^2) \quad \text{as } n \to \infty\]
<center>Rewriting this, we can say that</center>

\[ \bar{X}_n \thicksim N( \mu, \frac{\sigma^2}{n} ) \quad \text{or} \quad \bar{X}_n \thicksim \mu + \frac{\sigma}{\sqrt{n}} Z  \]


(2) <b>LLN</b>: As sample size n increases, sample mean converges to expected value. AKA "convergence to probability", the deviation of X from its mean tends to zero as n goes to infinity. <br>
\[\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty\]
\[\mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{for any } \epsilon > 0 \text{ as } n \to \infty\]
<br>
<div style="margin-left: 20px;">
    <b>Sample Moments Using LLN</b><br>
    - If \( X_i \) are iid and \( Var(X_i^k) < \infty \), sample moments are consistent estimators of the population k'th moment
    \[
    \frac{1}{n} \sum_{i=1}^{n} X_i^k \quad \xrightarrow{p} \quad \mathbb{E}[X^k] \quad \text{as} \quad n \to \infty
    \]
</div>

<br><br>
<b>Markov's Inequality</b>: <br>
- Markov tells us that probability of X >= a can never exceed the mean of X divided by a <br>
- Suppose a = 100 * E(X), then P(|X| > 100 E(X) ) <= 1 / 100 (so the probability that X is 100 times greater than its mean must not exceed 1 / 100

\[ \mathbb{P}(|X| \geq a) \leq \frac{\mathbb{E}[X]}{a} \quad \text{for } a > 0 \]

<b>Chebyshev's Inequality</b>: <br>
- Quantifies how much the spread of a distribution can vary <br>
- Suppose k = 4, then the right side is 1/16. The probability that X lands 4 times away from its SD is less than 1/16.
\[ \mathbb{P}(|X - E[X]| \geq k) \leq \frac{Var(X)}{k^2} \quad \text{for } k > 0 \]
\[ \mathbb{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \quad \text{for } k > 0  \]

<div style="margin-left: 20px;">
<b>Proof of Weak Law of Large Numbers using Chebyshev's Inequality </b><br>
Show that sample mean \( \bar{X}_n \) converges in probability to \( \mu \) as \( n \to \infty \), i.e.,
\[
\forall \epsilon > 0, \lim_{n \to \infty} \mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) = 0
\]

Apply this to the sample mean \( \bar{X}_n \). Since \( \mathbb{E}[\bar{X}_n] = \mu \) and \( \text{Var}(\bar{X}_n) = \frac{\sigma^2}{n} \), we have:
\[
\mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\text{Var}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2}
\]

As \( n \to \infty \), the right-hand side \( \frac{\sigma^2}{n \epsilon^2} \) tends to 0. Therefore,
\[
\lim_{n \to \infty} \mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) = 0
\]
</div>


<br><br>
<h4>Convergence Theorems</h4>
- Besides using <br>
(1) LLN for \( \xrightarrow{p} \) <br>
(2) CLT for \( \xrightarrow{d} \)
<br><br>

<b>1. Continuous Mapping Theorem (CMT) </b><br>
The CMT allows us to apply FIXED continuous functions to converging sequences of random variables. <br>
<center> If \(X_n \xrightarrow{p} X\), and \(g(\cdot)\) is a fixed continuous function, then: </center>
\[
g(X_n) \xrightarrow{p} g(X)
\]

<br><br>

<b>2. Delta Method</b><br>
<center>If \(X_n\) is a sequence of random variables and \(g(\cdot)\) is differentiable: </center>
\[
\sqrt{n}(X_n - \theta) \xrightarrow{d} N(0, \sigma^2),
\]

\[
\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{d} N(0, \sigma^2 [g'(\theta)]^2)
\]

<br><br>

<b>3. Slutsky's Theorem</b><br>
If there is convergence in BOTH distribution and probability (to a constant):
<center>Let \(X_n \xrightarrow{d} X\) and \(Y_n \xrightarrow{p} c\). Then: </center>
\[ \text{(1)} \quad X_n + Y_n \xrightarrow{d} X + c\]
\[\text{(2)} \quad X_n Y_n \xrightarrow{d} cX\]
\[\text{(3)} \quad \frac{X_n}{Y_n} \xrightarrow{d} \frac{X}{c}\]



<br><br><br><br>
<h2>II. Statistics of Sampling</h2><br>

<h4>Confidence Interval</h4>
<center><img src = "/assets/confidence.png" width = 400px> </center> <br>

\[\mathbb{P}\left(-z_{\frac{\alpha}{2}} \leq \frac{\bar{X} - \mu}{\sigma_{\bar{X}}} \leq z_{\frac{\alpha}{2}}\right) = 1 - \alpha\]
\[\mathbb{P}\left( \bar{X} - z_{\frac{\alpha}{2}}\sigma_{\bar{X}} \leq \mu \leq \bar{X} + z_{\frac{\alpha}{2}}\sigma_{\bar{X}} \right) = 1 - \alpha\]

<br>
When \( \sigma \) is <b>KNOWN</b>, we have \( \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} \). <br>
But if \( \sigma \) us <b>UNKNOWN</b>  , we can obtain an estimator:
\[ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2  \quad \quad \text{(sample variance)} \]
<br>

<b>Sample Variance</b><br>
- Sample variance \( \hat{\sigma}^2 \) is <b>biased</b> estimator of population variance \( \sigma^2 \). <br>
- Intuitively, we know that \( X_i \) is closer to \( \bar{X} \) than the true mean so variance is lower, so the below formulas make sense
\[ \mathbb{E}[\hat{\sigma}^2] = \frac{n-1}{n} \sigma^2 \]


To obtain an <b>unbiased</b> estimator, we adjust by multiplying the inverse so we can get:
\[ S^2 = \frac{n}{n-1} \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \]
\[ \mathbb{E}[\hat{\sigma}^2] = \sigma^2  \]

* note that these are for sampling with replacement (WR). <br>
Use <b>Finite Population Correction</b> for sampling WITHOUT replacement <br>
\[(1 - \frac{n-1}{N-1})\]

<br><br>
<b>Differences between SD and SE</b>
\[ SE(\bar{X}) = \frac{S}{\sqrt{n}} = \frac{\sigma}{\sqrt{n}} \]
\[ SD(\bar{X}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2} \]

<br><br>

<h4> Chi-Squared \( \chi^2 \) and \( t\)  </h4><br>
<b>(1) Chi-Squared</b> <br>
- The Chi Squared distribution is represented by the <b>sum</b> of independent squared Normal random variables with n degrees of freedom:
\[X \sim \chi^2_n \quad \text{where} \quad X = Z_1^2 + Z_2^2 + \dotsc Z_n^2 \quad \text{and} \quad Z \sim N(0,1)\]
<center>and actually</center>
\[\chi^2_n \sim \text{Gamma}(\frac{n}{2} , \frac{1}{2}) \]
<center>CLT tells us</center>
\[\chi^2_n \sim N(n, 2n) \]
* Note if \( U \sim \chi^2_{n} \) and \( V \sim \chi^2_{m} \), and U and V are independent, then \( U + V \sim \chi^2_{n + m} \)
<br><br><br>

<b>(2) t-Distribution </b> <br>
The t-distribution is useful for making inferences based on small sample sizes. If Z and X are independent:
\[
t = \frac{Z}{\sqrt{\frac{X}{n}}}, \quad Z \sim N(0,1), \quad X \sim \chi^2_n
\]

* \( t_n \xrightarrow{d} Z \sim N(0,1) \) as n goes to infinity. <br>
* t has a fatter tail distribution than Z, and as the n degrees of freedom increases, it slims and looks more like a Normal <br><br>

Interested in the quantity
\[
\frac{\bar{X} - \mu}{SE(\bar{X})} \sim t_{n-1} =
\]


<br>
<h4> Stats from Normal Population  </h4><br>
<center><img src = "/assets/normal.png" width = 700px> </center> <br>

<br><br>
<h4>Multivariate Normal</h4>
A random vector \( \mathbf{X} \) follows a multivariate normal distribution if: (A is p x q)
\[ \mathbf{X}  = AZ + \mu  \quad \sim \quad \mathcal{N}_p(\mathbf{\mu}, \Sigma) \quad \quad (\Sigma = A A^T) \]
\[ \text{where} \quad \quad Z \sim N_q(0, I) \quad \quad \text{is a spherical normal} \]

(1) \( \mathbb{E}[X] = A*E[Z] + \mu = \mu \in \mathbb{R}^p \) <br>
(2) \( \text{Var}(X) = \text{Cov}(X) = \Sigma \in \mathbb{R}^{p \times p} \) where \( \Sigma_{i,j} = \text{Cov}(X_i, X_j) \). <br>


<br><br>
<h4>Cumulant Generating Functions and Kurtosis/Skew</h4><br>
Let \( K(t) = log(M_X(t)) \) be the cumulant generating function of X and \( k_n = K^{(n)}(0) \)
\[ \text{Skew}(X) = \frac{k_3}{k^{3/2}_2} = \frac{\mathbb{E}[(X - \mu)^3]}{\sigma^3} \]
\[ \text{Kurt}(X) = 3 + \frac{k_4}{k^{2}_2} = \frac{\mathbb{E}[(X - \mu)^4]}{\sigma^4} \]






