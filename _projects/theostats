---
layout: page

title: theoretical statistics

description: course notes from STATS200

img:

importance: 3

category: work
---
<br><br>

<h2>I. Probability Review</h2><br>
<h4> Laws of Probability Theory </h4>

(1) De Morgans Laws: <br>
$$(A \cup B \cup C)^c = A^c \cap B^c \cap C^c$$

(2) Law of Total Probability: <br>
- prob of event A occuring is equal to sum of event A occurring in all possible scenarios
$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$$

(3) Inclusion-Exclusion Principle: <br>
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

<b>Conditional Probability:</b> <br>
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
$$P(A \cap B) = P(A|B)P(B) = {}^* P(B|A)P(A) $$
<center> * Bayes Theorem! </center><br>

(1) Marginal Independence: <br>
- A and B are marginally independent if: <br>
$$P(A|B) = P(A)$$
$$P(A \cap B) = P(A)P(B)$$

(2) Conditional Independence: <br>
- A1 and A2 are conditionally independent given B if:
$$P(A_1 \cap A_2 | B) = P(A_1|B)P(A_2|B)$$



<br><br>
<h4>Random Variables & Distributions</h4>

<b>CDFs and PDFs: </b> <br>
- By FTC, the PDF is equal to the derivative of the CDF <br>
- Integrating the PDF from -∞ to x gives us the CDF
$$F_X(x) = P(X \leq x)  = \int_{-\infty}^{x} f(t) \, dt$$
$$f_X(x) = \frac{d}{dx} F(x)$$


<b>Expectation: </b> <br>
$$\mathbb{E}[X] = \sum_{i} x_i P(X = x_i) \quad \text{(for discrete variables)}$$
$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx \quad \text{(for continuous variables)}$$
(1) Linearity Rule:
$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$
(2) Sum Rule:
$$\mathbb{E}\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} \mathbb{E}[X_i]$$

(3) Expectation of a Function: <br>
\[ E[g(X)] = \sum_{x} g(x) P(X = x) \]
\[ E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) \,dx \]


(4). Independence:
\[ \text{If X and Y are independent} \rightarrow \quad E[XY] = E[X] \cdot E[Y] \]




<b>Variance: </b> <br>

$$\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

$$\text{Var}(cX) = c^2 \text{Var}(X)$$

$$\text{Var}(aX + bY) = a^2\text{Var}(X) + b^2\text{Var}(Y) + 2ab\text{Cov}(X,Y)$$

<br><br>
<h4>Joint Variables</h4>
- <b>Marginal Densities</b> tell us the probability of one variable by ignore the other (if given a joint dist. of weight and height, the marginal of weight tells us info on weight regardless of height)
<br>
- <b>Conditional Densities</b> tell us the probability of one variable given that you know the value of the other (if height is 5'6, whats prob that weight is 110?)

<br><br>
<b>Discrete</b>: <br>

(1) Joint PMF:
\[ P(X = x, Y = y) = p_{X,Y}(x,y) \]

(2) Joint CDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \sum_{x' \leq x} \sum_{y' \leq y} p_{X,Y}(x', y') \]

(3) Marginal PMFs can be obtained from the joint PMF:
\[ p_X(x) = \sum_{y} p_{X,Y}(x,y) \]
\[ p_Y(y) = \sum_{x} p_{X,Y}(x,y) \]

<br>
<b>Continuous</b>: <br>

(1) Joint CDF is obtained by integrating the PDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \iint_{-\infty}^{(x,y)} f_{X,Y}(u,v) \,du\,dv \]

(2) Marginals obtained by integrating over opposite variable:
\[ f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dy \]
\[ f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dx \]


<br>
<b>Covariance: </b> <br>
\[ \text{Cov}(X, Y) = E\left[(X - E[X])(Y - E[Y])\right] \]

\[ \text{Cov}(X, Y) = E[XY] - E[X]E[Y] \]


(1) Covariance of a Constant:
\[ \text{Cov}(X, c) = 0 \]

(2) Covariance of a Random Variable with Itself:
\[ \text{Cov}(X, X) = \text{Var}(X) \]

(3) Covariance of a Linear Combination:
\[ \text{Cov}(aX + bY, Z) = a \, \text{Cov}(X, Z) + b \, \text{Cov}(Y, Z) \]

(4) Correlation:
\[ \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \quad \sigma = \text{sd} \quad \sigma^2 = \text{var} \]



<br><br>
<h4>Moment Generating Functions (MGFs)</h4>
MGFs can help us find the distributions of SUMS of independent RVs. <br><br>
(1) Zero'th moment is always = 1 <br>
(2) First moment is always = \( E(X) \) <br>
(3) Second moment is \( E(X^2) \) and can tell us the variance \( \sigma^2 = E(X^2) - [E(X)]^2 \) <br>


\[ M_X(t) = \mathbb{E}[e^{tX}] = \begin{cases}
\sum e^{t x} p(x)  \quad \text{(discrete)} \\
\\
\int_{}^{} e^{t x} f(x) \, dx \quad \text{(continuous)}
\end{cases}
\]

(1) MGF of the sum of independent RVs is the product
\[ M_{X_1 + X_2 + \cdots + X_n}(t) = M_{X_1}(t) M_{X_2}(t) \cdots M_{X_n}(t) \]
<center>  because </center>
\[ \mathbb{E}[e^{t(X_1 + X_2)}] = \mathbb{E}[e^{tX_1}] \cdot \mathbb{E}[e^{tX_2}] \]


(2) The k'th Derivative is the k'th Moment:
\[\mathbb{E}[X^k] = M_X^{(k)}(0) \]


(3) Continuity Theorem: <br>
<center>X_n and X are real-valued random variables:</center>

\[ X_n \xrightarrow{d} X \quad \text{as} \quad n \to \infty \quad (\text{convergence in distribution}) \]

<center>if and only if </center>

\[ M_{X_n}(t) \to M_X(t) \quad \text{as} \quad n \to \infty \quad \text{for all} \quad t. \]


<br><br>
<h4>Transformations of Random Variables</h4>
(1) We know some pdf \( f_X (x) \) <br>
(2) We know some function \( Y = g(X) \) (where g is continuous and strictly monotone) <br>
(3) We want to find the pdf \( f_Y (y) \) <br>
(4) Find the inverse of \( Y = g(X) \), so solve for X <br>
(5) Find the derivative of (4) <br>
(6) Plug in \( f_Y (y) = f_X (4) * |5| \)
<br><br>






<b> Application: Generating iid RVs ~F from Uniform RVs! </b> <br>
(1) Lets say I want to generate X1, X2,... Xn iid from some general distribution function F (given F is continuous and strictly increasing) <br>
(2) We can sample U1, U2... Un iid from Unif(0,1) <br>
(3) Then generate Xi = F^-1(Ui) <br><br>


Choose the distribution \( X \) you want to sample from (e.g., exponential, uniform, normal, etc.).

Determine the CDF \( F_X(x) \) of the desired distribution.

Set the uniform random variable \( U \) equal to the CDF:
\[U = F_X(x)\]

Rearrange the equation to solve for \( x \) in terms of \( U \):
\[x = F_X^{-1}(U)\]

Use a random number generator to obtain values of \( U \) and apply the above equation to generate samples from the desired distribution. Pretty damn cool if you ask me.

<br><br><br><br>
<h4>Central Limit Theorem (CLT) & Law of Large Numbers (LLN)</h4>

(1) <b>CLT</b>: The sum of a large number of i.i.d RV's tend to follow a normal distribution. AKA "convergence in distribution" to a normal. <br>
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \quad Var(\bar{X}_n) = \frac{\sigma^2}{n} \]
\[ \sqrt{n} ( \bar{X}_n - \mu) \quad \xrightarrow{d} \quad \mathcal{N}(0, \sigma^2) \quad \text{as } n \to \infty\]
<center>Rewriting this, we can say that</center>

\[ \bar{X}_n \thicksim N( \mu, \frac{\sigma^2}{n} ) \quad \text{or} \quad \bar{X}_n \thicksim \mu + \frac{\sigma}{\sqrt{n}} Z  \]


(2) <b>LLN</b>: As sample size n increases, sample mean converges to expected value. AKA "convergence to probability", the deviation of X from its mean tends to zero as n goes to infinity. <br>
\[\bar{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty\]
\[\mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{for any } \epsilon > 0 \text{ as } n \to \infty\]
<br>
<div style="margin-left: 20px;">
    <b>Sample Moments Using LLN</b><br>
    - If \( X_i \) are iid and \( Var(X_i^k) < \infty \), sample moments are consistent estimators of the population k'th moment
    \[
    \frac{1}{n} \sum_{i=1}^{n} X_i^k \quad \xrightarrow{p} \quad \mathbb{E}[X^k] \quad \text{as} \quad n \to \infty
    \]
</div>

<br><br>
<b>Markov's Inequality</b>: <br>
- Markov tells us that probability of X >= a can never exceed the mean of X divided by a <br>
- Suppose a = 100 * E(X), then P(|X| > 100 E(X) ) <= 1 / 100 (so the probability that X is 100 times greater than its mean must not exceed 1 / 100

\[ \mathbb{P}(|X| \geq a) \leq \frac{\mathbb{E}[X]}{a} \quad \text{for } a > 0 \]

<b>Chebyshev's Inequality</b>: <br>
- Quantifies how much the spread of a distribution can vary <br>
- Suppose k = 4, then the right side is 1/16. The probability that X lands 4 times away from its SD is less than 1/16.
\[ \mathbb{P}(|X - E[X]| \geq k) \leq \frac{Var(X)}{k^2} \quad \text{for } k > 0 \]
\[ \mathbb{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \quad \text{for } k > 0  \]

<div style="margin-left: 20px;">
<b>Proof of Weak Law of Large Numbers using Chebyshev's Inequality </b><br>
Show that sample mean \( \bar{X}_n \) converges in probability to \( \mu \) as \( n \to \infty \), i.e.,
\[
\forall \epsilon > 0, \lim_{n \to \infty} \mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) = 0
\]

Apply this to the sample mean \( \bar{X}_n \). Since \( \mathbb{E}[\bar{X}_n] = \mu \) and \( \text{Var}(\bar{X}_n) = \frac{\sigma^2}{n} \), we have:
\[
\mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\text{Var}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2}
\]

As \( n \to \infty \), the right-hand side \( \frac{\sigma^2}{n \epsilon^2} \) tends to 0. Therefore,
\[
\lim_{n \to \infty} \mathbb{P}(|\bar{X}_n - \mu| \geq \epsilon) = 0
\]
</div>


<br><br>
<h4>Convergence Theorems</h4>
- Besides using <br>
(1) LLN for \( \xrightarrow{p} \) <br>
(2) CLT for \( \xrightarrow{d} \)
<br><br>

<b>1. Continuous Mapping Theorem (CMT) </b><br>
The CMT allows us to apply FIXED continuous functions to converging sequences of random variables. <br>
<center> If \(X_n \xrightarrow{p} X\), and \(g(\cdot)\) is a fixed continuous function, then: </center>
\[
g(X_n) \xrightarrow{p} g(X)
\]

<br><br>

<b>2. Delta Method</b><br>
If \(X_n\) is a sequence of random variables and \(g(\cdot)\) is differentiable:
\[\sqrt{n}(X_n - \theta) \xrightarrow{d} N(0, \sigma^2)\]
\[\sqrt{n}(g(X_n) - g(\theta)) \xrightarrow{d} N(0, \sigma^2 [g'(\theta)]^2)\]
<br>
Or we can also say that if \( \mathbb{E}[X_n] = \mu \) and \( Var(X_n) \) is small, then:
\[ \mathbb{E}[f(X_n)] = f(\mu) \]
\[  Var(f(X_n)) = \sigma^2(f'(\mu))^2 \]


<br><br>

<b>3. Slutsky's Theorem</b><br>
If there is convergence in BOTH distribution and probability (to a constant):
<center>Let \(X_n \xrightarrow{d} X\) and \(Y_n \xrightarrow{p} c\). Then: </center>
\[ \text{(1)} \quad X_n + Y_n \xrightarrow{d} X + c\]
\[\text{(2)} \quad X_n Y_n \xrightarrow{d} cX\]
\[\text{(3)} \quad \frac{X_n}{Y_n} \xrightarrow{d} \frac{X}{c}\]



<br><br><br><br>
<h2>II. Statistics of Sampling</h2><br>

<h4>Confidence Interval</h4>
<center><img src = "/assets/confidence.png" width = 400px> </center> <br>

\[\mathbb{P}\left(-z_{\frac{\alpha}{2}} \leq \frac{\bar{X} - \mu}{\sigma_{\bar{X}}} \leq z_{\frac{\alpha}{2}}\right) = 1 - \alpha\]
\[\mathbb{P}\left( \bar{X} - z_{\frac{\alpha}{2}}\sigma_{\bar{X}} \leq \mu \leq \bar{X} + z_{\frac{\alpha}{2}}\sigma_{\bar{X}} \right) = 1 - \alpha\]

<br>
When \( \sigma \) is <b>KNOWN</b>, we have \( \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} \). <br>
But if \( \sigma \) us <b>UNKNOWN</b>  , we can obtain an estimator:
\[ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2  \quad \quad \text{(sample variance)} \]
<br>

<b>Sample Variance</b><br>
- Sample variance \( \hat{\sigma}^2 \) is <b>biased</b> estimator of population variance \( \sigma^2 \). <br>
- Intuitively, we know that \( X_i \) is closer to \( \bar{X} \) than the true mean so variance is lower, so the below formulas make sense
\[ \mathbb{E}[\hat{\sigma}^2] = \frac{n-1}{n} \sigma^2 \]


To obtain an <b>unbiased</b> estimator, we adjust by multiplying the inverse so we can get:
\[ S^2 = \frac{n}{n-1} \hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \]
\[ \mathbb{E}[\hat{\sigma}^2] = \sigma^2  \]

* note that these are for sampling with replacement (WR). <br>
Use <b>Finite Population Correction</b> for sampling WITHOUT replacement <br>
\[(1 - \frac{n-1}{N-1})\]

<br><br>
<b>Differences between SD and SE</b>
\[ SE(\bar{X}) = \frac{S}{\sqrt{n}} = \frac{\sigma}{\sqrt{n}} \]
\[ SD(\bar{X}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2} \]

<br><br>

<h4> Chi-Squared \( \chi^2 \) and \( t\)  </h4><br>
<b>(1) Chi-Squared</b> <br>
- The Chi Squared distribution is represented by the <b>sum</b> of independent squared Normal random variables with n degrees of freedom:
\[X \sim \chi^2_n \quad \text{where} \quad X = Z_1^2 + Z_2^2 + \dotsc Z_n^2 \quad \text{and} \quad Z \sim N(0,1)\]
<center>and actually</center>
\[\chi^2_n \sim \text{Gamma}(\frac{n}{2} , \frac{1}{2}) \]
<center>CLT tells us</center>
\[\chi^2_n \sim N(n, 2n) \]
* Note if \( U \sim \chi^2_{n} \) and \( V \sim \chi^2_{m} \), and U and V are independent, then \( U + V \sim \chi^2_{n + m} \)
<br><br><br>

<b>(2) t-Distribution </b> <br>
The t-distribution is useful for making inferences based on small sample sizes. If Z and X are independent:
\[
t = \frac{Z}{\sqrt{\frac{X}{n}}}, \quad Z \sim N(0,1), \quad X \sim \chi^2_n
\]

* \( t_n \xrightarrow{d} Z \sim N(0,1) \) as n goes to infinity. <br>
* t has a fatter tail distribution than Z, and as the n degrees of freedom increases, it slims and looks more like a Normal <br><br>

Interested in the quantity
\[
\frac{\bar{X} - \mu}{SE(\bar{X})} \sim t_{n-1} =
\]


<br>
<h4> Stats from Normal Population  </h4><br>
<center><img src = "/assets/normal.png" width = 700px> </center> <br>

<br><br>
<h4>Multivariate Normal</h4>
A random vector \( \mathbf{X} \) follows a multivariate normal distribution if: (A is p x q)
\[ \mathbf{X}  = AZ + \mu  \quad \sim \quad \mathcal{N}_p(\mathbf{\mu}, \Sigma) \quad \quad (\Sigma = A A^T) \]
\[ \text{where} \quad \quad Z \sim N_q(0, I) \quad \quad \text{is a spherical normal} \]

(1) \( \mathbb{E}[X] = A*E[Z] + \mu = \mu \in \mathbb{R}^p \) <br>
(2) \( \text{Var}(X) = \text{Cov}(X) = \Sigma \in \mathbb{R}^{p \times p} \) where \( \Sigma_{i,j} = \text{Cov}(X_i, X_j) \). <br>


<br><br>
<h4>Method of Moments</h4>
Goal: We have some data. We understand the properties of some known distributions. Let us describe the data as generated from some natural distribution.
One way to describe distributions can be through its moments. For instance: <br><rb>
- the first moment (mean) tells the central location <br>
- the second moment shows the spread <br>
- higher moments like skew and kurtosis describe the shape <br><br>

Typically, the number of parameters we want to estimate = the number of moments we can find. We can express as a function of the moments:
\[ \mu_k = \mathbb{E}[X^{(k)})] \]

Suppose we want to estimate two parameters: The general form is
\[ \theta_1 = f_1(\mu_1, \mu_2) \]
\[ \theta_2 = f_2(\mu_1, \mu_2) \]

but we want to estimate it using sample moments (bc we usually dont have access to the whole population)
\[ \hat{\mu_k} = \frac{1}{n} \sum{X^{(k)}_i} \]
\[ \hat{\theta_1} = f_1(\hat{\mu_1}, \hat{\mu_2}) \]
\[ \hat{\theta_2} = f_2(\hat{\mu_1}, \hat{\mu_2}) \]

<h5>Sampling Distributions of MOMs</h5>
Goal: We want to see how stable our estimated parameters are. To do so, we analyze it's sampling distributions (mean, variance, SE) so we can also construct confidence intervals.
There are two ways we can see the sampling distributions of these parameter estimates: bootstrap sampling and large sample approximations <br><br>

(1) By bootstrap simulation, we can estimate standard error as the following <br>
- Sample n datapoints like 1000 times <br>
- Each iteration from 1 to 1000 provides us with an estimate of parameter p*

\[ SE_{\hat{p}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (p^*_i - \bar{p})^2} \]

<br>
(2) Large sampling approximation is based on consistency (LLN): as the number of \( \hat{\theta} \) increase, the estimators become more correct to a true estimates, as well as asymptotic normality (CLT). <br>
- See delta method

<br><br><br>
<b>Cumulant Generating Functions and Kurtosis/Skew</b><br>
Let \( K(t) = log(M_X(t)) \) be the cumulant generating function of X and \( k_n = K^{(n)}(0) \)
\[ \text{Skew}(X) = \frac{k_3}{k^{3/2}_2} = \frac{\mathbb{E}[(X - \mu)^3]}{\sigma^3} \]
\[ \text{Kurt}(X) = 3 + \frac{k_4}{k^{2}_2} = \frac{\mathbb{E}[(X - \mu)^4]}{\sigma^4} \]


<br><br>
<h4>Maximum Likelihood</h4>
Say we have datapoints \(X_1 \ldots X_n \) with a joint density function \( f(x_1 \ldots x_n | \theta) \). Our goal is to find the parameters \( \theta \) so that we maximize the probability of observing these datapoints \( X_i \). <br><br>

If they are assumed to be iid, then the likelihood function is considered as a product of the individual densities of X given \( \theta \).
But its easier to take the <b>log likelihood</b> since multiplying a bunch of probabilities < 1 will go to zero
\[ l(\theta) = \sum{logf(X_i | \theta)} \]

To find the \( \theta \) that maximizes the log likelihood function, we take its first derivative (or partial derivations) wrt the unknown parameter and set = 0 and solve for \(\theta \).
\[ \text{score function} \quad l'(\theta) = 0 \]

Local/Global Max: We can also (further) ensure that the root is at least a local max by checking \( l''(\hat{\theta}) < 0 \), or global for ex if the function is also concave. <br><br>
<b>Likelihood Ratio</b> <br>
- How much more probable it is that our estimated parameter is = to x than opposed to some other value y. <br><br>
\[ \frac{l(\hat{\theta} = x)}{l(\hat{\theta} = y)} \]

<b>Sampling distribution of MLE</b><br>
- We can follow a similar approach as MOM (either bootstrapping or large sample approx) <br>
- The SE of MLE is generally smaller and more optimal compared to other estimators like MOMs <br>
- Under <b>regularity conditions</b> (where the bounds is NOT dependent on \( \theta \) -- so Unif(0,\(\theta\)) would be invalid) then:
\[ \mathbb{E}[l'(\theta)] = 0 \]
\[ \mathbb{E}[-l''(\theta)] = I(\theta) \]

<b>Fisher Information \( I(\theta) \)</b><br>
- The amount of information that our data can provide about our true parameter <br>
- For n iid RVs, the Fisher Info of the total sample is \( I_{n}(\theta) = n I(\theta)\) <br>
- Inversely proportional to variance under asymptotic normality and regularity conditions \( Var(\hat{\theta}) = \frac{1}{I(\theta)} \) <br>
\[  I(\theta) = \mathbb{E}[-l''(\theta)]  \]

<br><br>
<h5>(1) Consistency of MLE (LLN)</h5>
If \(X_1 \ldots X_n\) are iid with a true value of \(\theta\)<br>
\[ \hat{\theta}_{MLE}(X_1 \ldots X_n) \xrightarrow{p} \theta \]

We can also show the relationship between the <b>KL divergence</b> and Fisher Information for small \(\hat{\theta} - \theta \)
\[ D_{\text{KL}}(\theta,\hat{\theta}) = \frac{1}{2} (\hat{\theta} - \theta)^2 I(\hat{\theta}) \]

<br>
<h5>(2) Asymptotic Normality of MLE (CLT)</h5>
*MLE is asymptotically efficient because the asymptotic variance is equal to bound given by Cramer Roa
\[ \sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N\left(0, \frac{1}{I(\theta)} \right) \]


<br><br>
<h5>Confidence Intervals for MLE</h5>
The distribution of \( \sqrt{nI(\theta)}(\hat{\theta} - \theta) \) is approximately Standard Normal  \( \sim N(0,1) \) <br>
\[ P(-z_{a/2} \leq \sqrt{nI(\theta)}(\hat{\theta} - \theta) \leq z_{a/2} ) = 1 - a \]
\[ \hat{\theta} \pm z_{a/2} \frac{1}{\sqrt{nI(\hat{\theta})}} \]



<br><br>
<h5>Comparing Estimators</h5>
We can assess which estimator (i.e., MOM vs MLE estimators) is better using the <b>Mean Squared Error</b>: <br>
\[ MSE(\hat{\theta}) = \mathbb{E}[(\hat{\theta}(X) - \theta)^2] = \int (\hat{\theta}(X) - \theta)^2 f(x|\theta) dx\]

<b>Variance-Bias Decomposition</b><br>
\[ MSE(\hat{\theta}) = \mathbb{E}[(\hat{\theta}(X) - \theta)^2] = Var(\hat{\theta}) + [\mathbb{E}\hat{\theta} - \theta]^2 \]


<br><br>
<h5>Efficiency & Cramer Roa Bound in MLE</h5>
<b>Cramer-Roa Bound</b>: the optimal (lowest possible) variance for some unbiased estimator. Functions as a benchmark against any estimator <br>

<br>
Suppose \( \bar{X} \sim N(\theta, \sigma_{n}^2) \) and \( \hat{\theta}_{a} = a\bar{X} \). We have \( Var(\hat{\theta}_{a}) = a^2 \sigma_{n}^2 \): <br>
<b> - a = 1 </b>  unbiased, all variance<br>
<b> - a = 0, </b>  all bias, no variance<br>

\[ MSE(\hat{\theta}_{a}, \theta) =  a^2 \sigma_{n}^2  + (1 - a)^2 \theta^2 \]

<br>
If \( \hat{\theta} \) is unbiased: \( E\hat{\theta} = \theta \), then \(  MSE(\hat{\theta}_{a}, \theta) = Var(\hat{\theta}) \). So to compare two unbiased estimators, we just focus on comparing their variances. <br>
\[ eff(\hat{\theta}_{1}, \hat{\theta}_{2}) = \frac{Var(\hat{\theta}_{1})}{Var(\hat{\theta}_{2})} \]

<br>
<b>Def: Cramer Roa Bound (CRB)</b><br>
If \( \hat{\theta} \) is an unbiased estimator of \( \theta \), then the minimum possible variance of that estimator is shown below. If it achieves this, the estimator is said to be <b>efficient</b> (for some fixed n)
\[ Var(\hat{\theta}) \geq \frac{1}{nI(\theta)} \]

<b>Folk Theorem</b>: <br>
- Shows MLE is asymptotically efficient<br>
Combining asymptotic normality and CRB, it follows that if a sequence of estimators such as \( T_n \) is consistent and asy. normal, then:
\[ \sqrt{n}(T_n - \theta) \xrightarrow{d} N(0, Var(\theta)) \]
\[ Var(\theta) \geq \frac{1}{I(\theta)} \]

And the approx CI is wider than the MLE interval, (showing MLE is optimal):
\[ T_n \pm z_{a/2} \frac{Var(T_n)}{\sqrt{n}} \]

<br><br>
<h4>Hypothesis Testing</h4>
<b>Simple Hypothesis</b>: All parameters are fully specified <br>
- H0 = Data is Normal \( \sim N(0,1) \) <br>
- Ha = Data is Poisson  \( \sim Pois(\lambda = 5) \) <br><br>

<b>Composite Hypothesis</b>: Some parameters are unknown <br>
- H0 = Data is Normal \( \sim N(\mu,\sigma^2) \) <br>
- Ha = Data is Poisson  \( \sim Pois(\lambda) \) <br><br>

<br>
<h5>Likelihood Ratio (for simple tests)</h5>
Tells us the probability of a null hypothesis compared to an alternative hypothesis.
\[ \frac{f(x | H_0)}{f(x | H_a)} \]

Reject \( H_0 \) when \( \frac{f(x | H_0)}{f(x | H_a)} < c \) <br>
- c is some constant that allows us to find X = some threshold to accept/reject (this is called the <b>test statistic</b>)<br>
- c controls tradeoff between probabilities two types of errors (i.e., accepting null when alt is true and vice versa)


<br><br><br>
<b>\( \alpha \) : Type I Error (False Positive):</b> Null is true but we reject null to accept alt <br>
- Choose some \( c \) to control \( \alpha \)
\[ P_{\mu = 0} (\bar{X} > c) = P(Z > c\sqrt{n}) = 1 - \Phi(c\sqrt{n}) = \alpha \]

<br>
<b> \( \beta \) : Type II Error (False Negative):</b> Alt is true but we reject alt to accept null <br>
- <b>Power</b> (True Negatives) = \( 1 - \beta \) (more power = less false negatives)
\[ P(Z > z_{\alpha} - \mu_1 \sqrt{n}) = 1 - \Phi(z_{\alpha} - \mu_1 \sqrt{n}) = 1 - \beta \]
\[ \beta = \Phi(z_{\alpha} - \mu_1 \sqrt{n})  = \Phi(-z_{\beta}) \] <br>

<br>
Discreteness <br>
In discrete cases, you can’t always hit your exact \( \alpha \). A theoretical fix is to add some randomness (like flipping another coin) to reach the exact level <br><br>

<br>
<b>Neyman-Pearson Lemma:</b> <br>
Justifies likelihood ratio as the optimal way to maximize power as the <b>uniformly most powerful test</b> - a hypothesis test which has the greatest power among all possible tests <br>
- One-Sided: Tests if \( \mu_0 \leq \mu_a \)<br>
- Two-Sided: Tests if \( \mu_0 \neq \mu_a \)<br>

<br>
<b> p-value </b><br>
Smallest significance value at which null would be rejected<br>
The confidence interval contains all those values for which the null hypothesis is accepted.

<br><br><br>
<h5>Generalized Likelihood Ratio GLR (for composite tests)</h5>
For composite tests, since params are unknown, we need to compute the "most likely" values using MLE
\[ \log \Lambda = \max_{\theta \in \omega_0} \ell(\theta) - \max_{\theta \in (\omega_0 \cup \omega_1)} \ell(\theta) \]

Reject \( H_0 \) for some \( \log \Lambda < c \) <br>
* GLR tests equal to two-sided z tests

<br><br>
<h6><b>Wilks Theorem</b></h6>
\( X_1, X_2, \dots, X_n \) are independent and identically distributed as \( f(x \mid \theta) \), where \( \theta \in \Omega \). Under regularity conditions, and if \( \theta \in \omega_0 \), then as \( n \to \infty \):
\[ -2 \log \Lambda \overset{d}{\to} \chi^2_{q - q_0} \]

where q = # of unknown parameters = dimensionality of parameter space

<br><br>
<h6><b>GLR Tests for Multinomial Distribution</b></h6>
- Binomial distribution but with more than two outcomes (rolling 6-sided die) <br>

1. Compute two MLE (restricted and unrestricted maxima) <br>
2. Restricted: \( \max_{\theta \in \omega_0} \ell(p(\theta)) \) <br>
3. Unrestricted: \( \max_{\theta \in \Omega} \ell(p) \) subject to \(\sum p_k = 1 \) <br>

\[ -2\log(\Lambda) = 2\sum_{j=1}^m O_j \log\left(\frac{O_j}{E_j}\right) \approx \chi^2_{m - q_0} \]
* \( O_j = n \hat{p_j} \) (Observed counts) <br>
* \( E_j = n p_j(\hat{\theta}) \) (Expected counts under H0 predicted by model)<br>

<br><br><br>
<h5>Comparing Two Samples</h5>
\[X_1, \ldots, X_n \overset{\text{iid}}{\sim} N(\mu_X, \sigma^2) \quad \quad
Y_1, \ldots, Y_m \overset{\text{iid}}{\sim} N(\mu_Y, \sigma^2)\]

\[\Delta = \mu_X - \mu_Y \quad \quad \hat{\Delta} = \bar{X} - \bar{Y}\]

\[\text{Var}(\hat{\Delta}) = \sigma^2 \left(\frac{1}{n} + \frac{1}{m}\right)\quad \quad \text{se}(\hat{\Delta}) = \sigma \sqrt{\frac{1}{n} + \frac{1}{m}} \]

If \(\sigma \) is known, then the confidence interval is:
\[ \hat{\Delta} \pm z_{\alpha/2} \cdot \text{se}(\hat{\Delta})\]

For unknown \( \sigma \) we have:
\[s_p^2 = \frac{(n-1)s_X^2 + (m-1)s_Y^2}{n + m - 2}\]
\[\widehat{\text{se}}(\hat{\Delta}) = s_p \sqrt{\frac{1}{n} + \frac{1}{m}}\]
\[t = \frac{\hat{\Delta} - \Delta}{\widehat{\text{se}}(\hat{\Delta})} \sim t_{\nu}, \quad \nu = n + m - 2\]
\[\hat{\Delta} \pm t_{\nu, \alpha/2} \cdot \widehat{\text{se}}(\hat{\Delta})\]



