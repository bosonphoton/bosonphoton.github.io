---
layout: page

title: theoretical statistics

description: course notes from STATS200

img:

importance: 8

category: work
---
<br><br>

<h2>I. Probability Review</h2><br>
<h4> Laws of Probability Theory </h4>

(1) De Morgans Laws: <br>
$$(A \cup B \cup C)^c = A^c \cap B^c \cap C^c$$

(2) Law of Total Probability: <br>
- prob of event A occuring is equal to sum of event A occurring in all possible scenarios
$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$$

(3) Inclusion-Exclusion Principle: <br>
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

<b>Conditional Probability:</b> <br>
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
$$P(A \cap B) = P(A|B)P(B) = {}^* P(B|A)P(A) $$
<center> * Bayes Theorem! </center><br>

(1) Marginal Independence: <br>
- A and B are marginally independent if: <br>
$$P(A|B) = P(A)$$
$$P(A \cap B) = P(A)P(B)$$

(2) Conditional Independence: <br>
- A1 and A2 are conditionally independent given B if:
$$P(A_1 \cap A_2 | B) = P(A_1|B)P(A_2|B)$$



<br><br>
<h4>Random Variables & Distributions</h4>

<b>CDFs and PDFs: </b> <br>
- By FTC, the PDF is equal to the derivative of the CDF <br>
- Integrating the PDF from -âˆž to x gives us the CDF
$$F_X(x) = P(X \leq x)  = \int_{-\infty}^{x} f(t) \, dt$$
$$f_X(x) = \frac{d}{dx} F(x)$$


<b>Expectation: </b> <br>
$$\mathbb{E}[X] = \sum_{i} x_i P(X = x_i) \quad \text{(for discrete variables)}$$
$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f_X(x) \, dx \quad \text{(for continuous variables)}$$
(1) Linearity Rule:
$$\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$$
(2) Sum Rule:
$$\mathbb{E}\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} \mathbb{E}[X_i]$$

(3) Expectation of a Function: <br>
\[ E[g(X)] = \sum_{x} g(x) P(X = x) \]
\[ E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) \,dx \]


(4). Independence:
\[ \text{If X and Y are independent} \rightarrow \quad E[XY] = E[X] \cdot E[Y] \]




<b>Variance: </b> <br>

$$\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

$$\text{Var}(cX) = c^2 \text{Var}(X)$$

$$\text{Var}(aX + bY) = a^2\text{Var}(aX) + b^2\text{Var}(bY) + 2ab\text{Cov}(X,Y)$$

<br><br>
<h4>Joint Variables</h4>
- <b>Marginal Densities</b> tell us the probability of one variable by ignore the other (if given a joint dist. of weight and height, the marginal of weight tells us info on weight regardless of height)
<br>
- <b>Conditional Densities</b> tell us the probability of one variable given that you know the value of the other (if height is 5'6, whats prob that weight is 110?)

<br><br>
<b>Discrete</b>: <br>

(1) Joint PMF:
\[ P(X = x, Y = y) = p_{X,Y}(x,y) \]

(2) Joint CDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \sum_{x' \leq x} \sum_{y' \leq y} p_{X,Y}(x', y') \]

(3) Marginal PMFs can be obtained from the joint PMF:
\[ p_X(x) = \sum_{y} p_{X,Y}(x,y) \]
\[ p_Y(y) = \sum_{x} p_{X,Y}(x,y) \]

<br>
<b>Continuous</b>: <br>

(1) Joint CDF is obtained by integrating the PDF:
\[ F_{X,Y}(x,y) = P(X \leq x, Y \leq y) = \iint_{-\infty}^{(x,y)} f_{X,Y}(u,v) \,du\,dv \]

(2) Marginals obtained by integrating over opposite variable:
\[ f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dy \]
\[ f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dx \]


<br>
<b>Covariance: </b> <br>
\[ \text{Cov}(X, Y) = E\left[(X - E[X])(Y - E[Y])\right] \]

\[ \text{Cov}(X, Y) = E[XY] - E[X]E[Y] \]


(1) Covariance of a Constant:
\[ \text{Cov}(X, c) = 0 \]

(2) Covariance of a Random Variable with Itself:
\[ \text{Cov}(X, X) = \text{Var}(X) \]

(3) Covariance of a Linear Combination:
\[ \text{Cov}(aX + bY, Z) = a \, \text{Cov}(X, Z) + b \, \text{Cov}(Y, Z) \]

(4) Correlation:
\[ \text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} \quad \sigma = \text{sd} \quad \sigma^2 = \text{var} \]



<br><br>
<h4>Moment Generating Functions (MGFs)</h4>
MGFs can help us find the distributions of SUMS of independent RVs.

\[ M_X(t) = \mathbb{E}[e^{tX}] = \begin{cases}
\sum e^{t x} p(x)  \quad \text{(discrete)} \\
\\
\int_{}^{} e^{t x} f(x) \, dx \quad \text{(continuous)}
\end{cases}
\]

(1) MGF of the sum of independent RVs is the product
\[ M_{X_1 + X_2 + \cdots + X_n}(t) = M_{X_1}(t) M_{X_2}(t) \cdots M_{X_n}(t) \]
<center>  because </center>
\[ \mathbb{E}[e^{t(X_1 + X_2)}] = \mathbb{E}[e^{tX_1}] \cdot \mathbb{E}[e^{tX_2}] \]


(2) The k'th Derivative is the k'th Moment:
\[\mathbb{E}[X^k] = M_X^{(k)}(0) \]



<br><br>
<h4>Central Limit Theorem (CLT) & Law of Large Numbers (LLN)</h4>

(1) <b>CLT</b>: The sum of a large number of i.i.d RV's tend to follow a normal distribution. AKA "convergence in distribution" to a normal. <br>
\[\overline{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i \quad \quad S_n = \sum_{i=1}^{n} X_i \]
\[ \sqrt{n} (\frac{S_n}{n} - \mu) \quad \xrightarrow{d} \quad \mathcal{N}(0, \sigma^2) \quad \text{as } n \to \infty\]
\[\frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \quad \xrightarrow{d} \quad \mathcal{N}(0, \sigma^2) \quad \text{as } n \to \infty\]

(2) <b>LLN</b>: As sample size n increases, sample mean converges to expected value. AKA "convergence to probability", the deviation of X from its mean tends to zero as n goes to infinity. <br>
\[\overline{X}_n \xrightarrow{p} \mu \quad \text{as } n \to \infty\]
\[\mathbb{P}(|\overline{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{for any } \epsilon > 0 \text{ as } n \to \infty\]



<br><br>
<b>Markov's Inequality</b>: <br>
- "What is the highest possible probability that X is greater than a, given that we know the mean of X"? <br>
- Markov tells us that probability of X >= a can never exceed the mean of X divided by a

\[ \mathbb{P}(|X| \geq a) \leq \frac{\mathbb{E}[X]}{a} \quad \text{for } a > 0 \]

<b>Chebyshev's Inequality</b>: <br>
- Quantifies how much the spread of a distribution can vary
\[ \mathbb{P}(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2} \quad \text{for } k > 0 \]


<br><br>
<h4>Transformations of Random Variables</h4>

<b> Application: Generating iid RVs ~F from Uniform RVs! </b> <br>
(1) Lets say I want to generate X1, X2,... Xn iid from some general distribution function F (given F is continuous and strictly increasing) <br>
(2) We can sample U1, U2... Un iid from Unif(0,1) <br>
(3) Then generate Xi = F^-1(Ui) <br><br>



Choose the distribution \( X \) you want to sample from (e.g., exponential, uniform, normal, etc.).

Determine the CDF \( F_X(x) \) of the desired distribution.

Set the uniform random variable \( U \) equal to the CDF:
\[U = F_X(x)\]

Rearrange the equation to solve for \( x \) in terms of \( U \):
\[x = F_X^{-1}(U)\]

Use a random number generator to obtain values of \( U \) and apply the above equation to generate samples from the desired distribution. Pretty damn cool if you ask me.

