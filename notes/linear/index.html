<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>matrix methods | Chelsea Zou</title> <meta name="author" content="Chelsea Zou"> <meta name="description" content="notes from linear algebra"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bosonphoton.github.io/notes/linear/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Chelsea </span><span class="font-weight-bold">Zou</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">notes</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/coolstuff/">cool stuff</a> </li> <li class="nav-item "> <a class="nav-link" href="/personal/">personal</a> </li> <li class="nav-item "> <a class="nav-link" href="/peculiarpeople/">peculiar people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">matrix methods</h1> <p class="post-description">notes from linear algebra</p> </header> <article> <style>body{font-size:16px;line-height:1}p{margin-bottom:8px}.math{font-size:15px}</style> <br><br> <h2> Vectors </h2> <br><br> <h4>Canonical Unit Vectors (standard basis vectors)</h4> \( \mathbb{e}_i\) = where the i-th position is 1 and all other positions are 0. For example, in \( \mathbb{R}^3 \): \[ \mathbf{e}_1 = (1, 0, 0), \quad \mathbf{e}_2 = (0, 1, 0), \quad \mathbf{e}_3 = (0, 0, 1) \] Any vector \( \mathbf{v} \in \mathbb{R}^n \) can be expressed as: \[ \mathbf{v} = v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + \cdots + v_n \mathbf{e}_n \] <br><br><br> <h4>Inner Products (dot products)</h4> \[ a \cdot b = a^T b \] <b>Properties: </b><br> 1. \( b^T a = a^T b \) <br><br> 2. \( (\gamma a)^T b = \gamma (a^T b)\) <br> scaling <b>a</b> then taking dot product with <b>b</b> is same as taking the dot product of <b>a</b> and <b>b</b> then scaling it <br><br> 3. \( (a + b)^T c = a^T c + b^T c \) <br> the dot product of <b>c</b> with the sum of <b>a</b> and <b>b</b> is the same as the dot product of (a and c) plus (b and c) <br><br> <br> <b>Operations: </b><br> 1. Picking out the i'th element: \( \quad e_{i}^T a = a_{i} \) <br><br> 2. Sum of elements: \( \quad 1_n a = a_1 + a_2 + a_3 ... a_n \) <br><br> 3. Sum of squares: \( \quad a^T a = a_{1}^2 + a_{2}^2 + a_{3}^2 ... a_{n}^2 \) \( \quad \quad a^T a = 0 \quad \text{iff} \quad a = 0 \) <br><br> <br><br><br> <h2> Linear Functions </h2> Satisfies superposition property (both homogeneity + additivity): <br><br> <b> 1. Homogeneity: \( f(ax) = a f(x) \) <br> 2. Additivity: \( f(x + y) = f(x) + f(y) \) </b><br><br> <b> Any linear function can be represented as a dot product \(f(x) = a^T x \) and any dot product function is linear (for some fixed vector a)</b> <br><br> \[ f(c_1 x + c_2 y) = \] \[ a^T (c_1 x + c_2 y) = a^T (c_1 x) + a^T (c_2 y) = c_1(a^T x) + c_2(a^T y) \] \[ = c_1 f(x) + c_2 f(y) \] <h4>Affine Functions</h4> Affine functions are linear functions \( f(x) = a^T x \) but offset by some b (linear always go through (0,0), affine functions need not): \[ f(x) = a^T x + b \] <h4>Taylor Approximation</h4> A first order taylor series is a linear approximation of the function f. <br> Using a nearby point z, we can approximate f(x) using: \[ \hat{f}(x) = f(z) + \Delta f(z)(x - z) \] <br><br><br> <h2> Norm and Distance </h2> \[ ||x|| = \sqrt{x_{1}^2 + x_{2}^2 ...} = \sqrt{x^T x} \] \[ ||x||^2 = \textbf{sum of squares} = x_{1}^2 + x_{2}^2 ... = x^T x \] Properties: <br><br> 1. \( ||cx|| = |c|||x|| \) <br> 2. \( ||x + y|| \leq ||x|| + ||y|| \)<br> 3. \( ||x|| \geq 0 \)<br> 4. \( ||x|| = 0 \quad \) iff \( \quad x = 0 \)<br><br><br> Mean square value: \( \frac{||x||^2}{n} \) <br> Root Mean square value (RMS): \( \frac{||x||}{\sqrt{n}} \quad \) (i.e., typical value of \( |x_i| \))<br><br><br> Block (stacked) Vectors: <br> \[ ||(a,b,c)||^2 = a^T a + b^T b + c^T c = ||a||^2 + ||b||^2 + ||c||^2 \] <br><br> <b>Chebyshev's Inequality: </b><br><br> - Judges the size of a vector <br> - Most numbers in a vector can't be much bigger than its RMS <br><br> Suppose k elements in vector x is larger than some a. Then: <br> - Number of elements \( |x_i| &gt; a \) is no more than \( ||x||^2 / a^2 \)<br> - The percentage of elements \( |x_i| \geq a RMS(x) \) is \( \leq \frac{1}{a^2} \) <br> \[ \frac{||x||^2}{a^2} \geq k \quad \text{or} \quad \frac{k}{n} \leq (\frac{RMS(x)}{a})^2 \]<br> <b>Distance: </b><br><br> Distance between a and b is: \( || a - b || \) <br><br> <b>Triangle Inequality</b>: Sum of twox sides must be greater than or equal to third side<br> <img src="/assets/tri.png" width="200px"> <br><br><br> <b>Cauchy Schwartz Inequality: \( |a \cdot b| \leq ||a||||b|| \) </b><br><br> <img src="/assets/cauchy.png" width="200px"> <br><br><br><br> Average of \( x = \frac{ x_1 + x_2 + ...}{n} = \frac{1^T x }{n} \) <br><br> Demeaned vector \( \tilde{x} = x - avg(x)1 \) <br><br> Standard deviation of \( x = rms(\tilde{x}) = \frac{||x - avg(x)1|| }{\sqrt{n}} \) <br><br> \( rms(x)^2 = avg(x)^2 + std(x)^2 \) <br><br> Standardized vector \( z = \frac{\tilde{x}}{std(x)}\) <br><br><br> <b>Example in Finance:</b> <br> x = time series of returns <br> avd(x) = average return <br> std(x) = volatility <br><br> Risk-Reward Plot: <br> <img src="/assets/finance.png" width="200px"> <br><br><br> <h2> Angles </h2> \[\theta = \cos^{-1}\left(\frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}\right)\] <img src="/assets/angle.png" width="700px"> <br><br><br><br><br> <h2>Linear Independence</h2> <br><br> Vectors \( \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\} \) are linearly independent if \( c_1, c_2, \dots, c_k \) <b>= 0</b> is the <b>ONLY</b> solution to: <br><br> \[ c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \dots + c_k \mathbf{v}_k = \mathbf{0}. \] <br><br><br> <h4>Basis</h4> <br> A set of vectors \( \{\mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n\} \) forms a basis for a vector space \( V \) if: <br><br> <ul> <li>The vectors are linearly independent.</li> <li>The vectors span the whole space \( V \) (i.e., any vector in \( V \) can be expressed as a linear combination of the basis vectors)</li> </ul> Ex: The vectors \((0,1)\) and \((1,0)\) span \(\mathbb{R}^2 \) <br><br><br><br><br> <h4>Orthonormal Expansion</h4> <br> - Orthonormal vectors are orthogonal and its norm is 1 <br><br> If \( \{\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_n\} \) is an orthonormal basis for a vector space \( V \), any vector \( \mathbf{v} \in V \) can be expressed as: <br><br> \[ \mathbf{v} = \sum_{i=1}^n (\mathbf{v} \cdot \mathbf{a}_i) \mathbf{a}_i, \] <br><br><br> <h4>Gram-Schmidt Process</h4> <br> Orthogonal vectors means they are linearly independent but <b>linearly independent vectors DOES NOT indicate they are orthogonal</b> <br> <br> Gram-Schmidt allows us to construct an orthonormal basis from a set of linearly independent vectors <br><br> Given a set of linearly independent vectors \( \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\} \), the Gram-Schmidt process generates an orthonormal set \( \{\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n\} \) as follows: <br><br> \[ \mathbf{u}_1 = \frac{\mathbf{v}_1}{\|\mathbf{v}_1\|} \] \[ \mathbf{u}_2 = \frac{\mathbf{v}_2 - (\mathbf{v}_2 \cdot \mathbf{u}_1) \mathbf{u}_1}{\| \mathbf{v}_2 - (\mathbf{v}_2 \cdot \mathbf{u}_1) \mathbf{u}_1 \|} \] \[ \mathbf{u}_3 = \frac{\mathbf{v}_3 - (\mathbf{v}_3 \cdot \mathbf{u}_1) \mathbf{u}_1 - (\mathbf{v}_3 \cdot \mathbf{u}_2) \mathbf{u}_2}{\| \mathbf{v}_3 - (\mathbf{v}_3 \cdot \mathbf{u}_1) \mathbf{u}_1 - (\mathbf{v}_3 \cdot \mathbf{u}_2) \mathbf{u}_2 \|} \] <br><br> This process continues for all \( \mathbf{v}_i \) to produce orthonormal vectors. <br><br><br><br> <h2>Matrices</h2> <br> <h5>Lower Triangular Matrices</h5> \[ \begin{bmatrix} a_{11} &amp; 0 &amp; 0 &amp; 0 \\ a_{21} &amp; a_{22} &amp; 0 &amp; 0 \\ a_{31} &amp; a_{32} &amp; a_{33} &amp; 0 \\ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} a_{11}x_1 \\ a_{21}x_1 + a_{22}x_2 \\ a_{31}x_1 + a_{32}x_2 + a_{33}x_3 \\ a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4 \end{bmatrix} \] <br><br> <h5>Difference Matrices</h5> \[ \begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; -1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; -1 &amp; 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = \begin{bmatrix} x_2 - x_1 \\ x_3 - x_2 \\ x_4 - x_3 \end{bmatrix} \] <br><br> <h5>Rotation Matrices</h5> A 2D rotation matrix for an angle $\theta$ is given by: \[ R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix} \] <br><br> <h5>Incidence Matrix</h5> <img src="/assets/incidence.png" width="300px"><br> For a simple undirected graph with m edges and n nodes, the incidence matrix \( A \in \mathbb{R}^{nodes \times edges} \) is defined as: \[ A_{i,j} = \begin{cases} 1, &amp; \text{if edge } j \text{ incoming to } i \\ -1, &amp; \text{if edge } j \text{ leaving } i \\ 0, &amp; \text{no connection} \end{cases} \] <br><br> <h5>Incidence Matrix</h5> A simple example of a 1D convolution smoothing matrix with a 3-point moving average kernel is: \[ C = \begin{bmatrix} \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \dots &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} \end{bmatrix} \] <br><br><br><br><br> <h2>Linear Mapping</h2> A <b>linear</b> mapping \( f: \mathbb{R}^n \to \mathbb{R}^m \) can be represented by a matrix \( A_{m \times n} \) such that for any vector \( x \in \mathbb{R}^n \): \[ f(x) = Ax \] <b> 1. f will always send 0 to 0 </b><br> If f(0) \( \neq \) 0, f is NOT linear <br> If f(0) \(=\) 0, f may or may not be linear <br><br> 2. \( f(\alpha x + \beta y) = A(\alpha x + \beta y) = \alpha Ax + \beta Ay \)<br><br> <b> 3. If f does not have a matrix representation, it is NONLINEAR</b><br><br><br><br> <h4>Finding A Given x</h4> For an \(n\)-vector \(\mathbf{x} = (x_1, \dots, x_n)\), we use the standard unit vectors to write: \[ \mathbf{x} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \cdots + x_n \mathbf{e}_n \] Since \(f\) is linear, we have: \[ f(\mathbf{x}) = x_1 f(\mathbf{e}_1) + x_2 f(\mathbf{e}_2) + \cdots + x_n f(\mathbf{e}_n) \] Now, we put together the \(m \times n\) matrix \(A\) column by column: \[ A = \left[ f(\mathbf{e}_1) \,\, f(\mathbf{e}_2) \,\, \cdots \,\, f(\mathbf{e}_n) \right] \] Thus, we can write: \[ f(\mathbf{x}) = x_1 f(\mathbf{e}_1) + x_2 f(\mathbf{e}_2) + \cdots + x_n f(\mathbf{e}_n) = A \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \] </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Chelsea Zou. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>